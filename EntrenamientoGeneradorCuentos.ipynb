{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPKWmVPoD5kh0xPNZN9PjAR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adrianaleticiamartinez/mcd_deep_learning/blob/main/EntrenamientoGeneradorCuentos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "g8rJpP4NU6iy"
      },
      "outputs": [],
      "source": [
        "# Importar las librerías necesarias\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import base64\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar el dispositivo para GPU si está disponible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "QfNXj-uAVMLo"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parámetros del modelo\n",
        "# ------------------------------\n",
        "# n_hidden: Número de unidades en las capas ocultas.\n",
        "n_hidden = 512\n",
        "\n",
        "# n_layers: Número de capas en el modelo LSTM.\n",
        "n_layers = 2\n",
        "\n",
        "# batch_size: Número de ejemplos que se procesan en cada paso de entrenamiento\n",
        "batch_size = 16\n",
        "#batch_size = 10\n",
        "\n",
        "# seq_length: Longitud de las secuencias de entrada (en términos de caracteres) que el modelo verá durante el entrenamiento.\n",
        "seq_length = 100\n",
        "\n",
        "# epochs: Número de veces que todo el conjunto de datos será utilizado para entrenar el modelo\n",
        "epochs = 150\n",
        "#epochs = 2\n",
        "\n",
        "# lr (learning rate): Tasa de aprendizaje, determina el tamaño de los pasos que da el modelo para ajustar sus pesos\n",
        "lr = 0.001\n",
        "\n",
        "# clip: Valor para \"recortar\" los gradientes y prevenir que se vuelvan demasiado grandes, lo que puede causar inestabilidad en el entrenamiento.\n",
        "clip = 5\n",
        "\n",
        "# top_k: Número de predicciones más probables a considerar al generar nuevo texto. Valores bajos generan texto más \"creativo\", pero menos coherente.\n",
        "top_k = 5\n"
      ],
      "metadata": {
        "id": "OOzwur_cVOVr"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocesamiento del texto: convertir el corpus a caracteres únicos y mapearlos a índices\n",
        "def preprocess_text(text):\n",
        "    chars = tuple(set(text))\n",
        "    int2char = dict(enumerate(chars))\n",
        "    char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "    encoded = np.array([char2int[ch] for ch in text])\n",
        "    return chars, int2char, char2int, encoded"
      ],
      "metadata": {
        "id": "OL-sZpdCVgUz"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el modelo LSTM\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, chars, n_hidden=n_hidden, n_layers=n_layers, drop_prob=0.5):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_layers = n_layers\n",
        "        self.n_chars = len(chars)\n",
        "\n",
        "        self.lstm = nn.LSTM(self.n_chars, n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(n_hidden, self.n_chars)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "        out = self.dropout(r_output)\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "JhY-X3H8VjLp"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para one-hot encoder\n",
        "def one_hot_encode(arr, n_labels):\n",
        "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    return one_hot"
      ],
      "metadata": {
        "id": "a-GjJxhUVli-"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear batches para el entrenamiento\n",
        "def get_batches(arr, batch_size, seq_length):\n",
        "    batch_size_total = batch_size * seq_length\n",
        "    n_batches = len(arr) // batch_size_total\n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y\n"
      ],
      "metadata": {
        "id": "iFe9AF5FVpuF"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para entrenar el modelo\n",
        "def train(model, data, epochs=epochs, batch_size=batch_size, seq_length=seq_length, lr=lr, clip=clip):\n",
        "    model.train()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        h = model.init_hidden(batch_size)  # Inicializa el estado oculto al comienzo de cada época\n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            x = one_hot_encode(x, model.n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Desvincular el estado oculto de su historial de gradientes anterior\n",
        "            h = tuple([each.detach() for each in h])\n",
        "\n",
        "            model.zero_grad()  # Resetear los gradientes\n",
        "            output, h = model(inputs, h)\n",
        "            loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
        "            loss.backward()\n",
        "\n",
        "            # Evitar la explosión de gradientes\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}/{epochs}... Loss: {loss.item()}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "617-H2okWAHK"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, char, device, h=None, top_k=5):\n",
        "\n",
        "    # Convertir el caracter a su índice entero correspondiente\n",
        "    x = np.array([[model.char2int[char]]])\n",
        "\n",
        "    # Codificar el índice entero en formato one-hot\n",
        "    x = one_hot_encode(x, model.n_chars)\n",
        "\n",
        "    # Convertir el array de NumPy a un tensor de PyTorch y moverlo al dispositivo especificado (CPU o GPU)\n",
        "    inputs = torch.from_numpy(x).to(device)\n",
        "\n",
        "    # Desactivar el cálculo del gradiente para la predicción\n",
        "    with torch.no_grad():\n",
        "        out, h = model(inputs, h)\n",
        "\n",
        "        # Aplicar softmax para obtener las probabilidades de salida\n",
        "        p = F.softmax(out, dim=1).data.cpu()\n",
        "\n",
        "        # Obtener los top k caracteres más probables y sus probabilidades\n",
        "        p, top_ch = p.topk(top_k)\n",
        "\n",
        "        # Convertir los top k caracteres y probabilidades a arrays de NumPy\n",
        "        top_ch = top_ch.numpy().squeeze()\n",
        "        p = p.numpy().squeeze()\n",
        "\n",
        "        # Elegir aleatoriamente el siguiente carácter de los top k caracteres basándonos en sus probabilidades\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "\n",
        "    # Devolver el carácter predicho y el estado oculto actualizado\n",
        "    return model.int2char[char], h"
      ],
      "metadata": {
        "id": "eEsV_LXs2Bdj"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample3(model, size, device, prime='A', top_k=top_k):\n",
        "    # Método para generar un nuevo texto basado en una secuencia inicial \"prime\".\n",
        "    # Esencialmente, esta función es un ciclo que llama al método \"predict\" definido anteriormente.\n",
        "    model.eval()  # Cambiar el modelo a modo de evaluación (eval mode)\n",
        "\n",
        "    # Calcular el estado oculto del modelo utilizando los caracteres iniciales (prime)\n",
        "    chars = [ch for ch in prime]  # Crear una lista de los caracteres iniciales\n",
        "    with torch.no_grad():  # Desactivar el cálculo del gradiente para hacer predicciones\n",
        "        # Inicializar el estado oculto con ceros al principio. El tamaño del batch es 1\n",
        "        # ya que deseamos generar una sola secuencia de texto.\n",
        "        h = model.init_hidden(batch_size=1)\n",
        "\n",
        "        # Procesar cada carácter de la secuencia inicial prime\n",
        "        for ch in prime:\n",
        "            char, h = predict(model, ch, device, h=h, top_k=top_k)\n",
        "\n",
        "        # Añadir el carácter generado a la secuencia\n",
        "        chars.append(char)\n",
        "\n",
        "        # Ahora, se toma el último carácter de la secuencia y se obtiene el siguiente.\n",
        "        # Este proceso se repite segun lo especificado\n",
        "        for ii in range(size):\n",
        "            char, h = predict(model, chars[-1], device, h=h, top_k=top_k)\n",
        "            chars.append(char)\n",
        "\n",
        "    # Unir la lista de caracteres\n",
        "    return ''.join(chars)"
      ],
      "metadata": {
        "id": "Lo-sjyn817WJ"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/adrianaleticiamartinez/mcd_deep_learning/refs/heads/main/cleaned_merged_fairy_tales_without_eos.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOmEGy1GZHiI",
        "outputId": "61caf67b-c8b2-43dd-a811-4009da4be597"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-26 06:04:34--  https://raw.githubusercontent.com/adrianaleticiamartinez/mcd_deep_learning/refs/heads/main/cleaned_merged_fairy_tales_without_eos.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20635335 (20M) [text/plain]\n",
            "Saving to: ‘cleaned_merged_fairy_tales_without_eos.txt’\n",
            "\n",
            "cleaned_merged_fair 100%[===================>]  19.68M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-09-26 06:04:35 (374 MB/s) - ‘cleaned_merged_fairy_tales_without_eos.txt’ saved [20635335/20635335]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el dataset y preprocesarlo\n",
        "\n",
        "\n",
        "with open('cleaned_merged_fairy_tales_without_eos.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "chars, int2char, char2int, encoded = preprocess_text(text)"
      ],
      "metadata": {
        "id": "mDX_haahWGdC"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar el modelo\n",
        "model = LSTM(chars, n_hidden=n_hidden, n_layers=n_layers).to(device)\n",
        "model.int2char = int2char\n",
        "model.char2int = char2int\n",
        "\n",
        "# Entrenar el modelo\n",
        "train(model, encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiIJE3XHWNev",
        "outputId": "dabb6b61-3fa6-4931-a889-55473f660c45"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/150... Loss: 1.370241641998291\n",
            "Epoch: 2/150... Loss: 1.3183226585388184\n",
            "Epoch: 3/150... Loss: 1.2729926109313965\n",
            "Epoch: 4/150... Loss: 1.25300133228302\n",
            "Epoch: 5/150... Loss: 1.2597562074661255\n",
            "Epoch: 6/150... Loss: 1.2478523254394531\n",
            "Epoch: 7/150... Loss: 1.24309241771698\n",
            "Epoch: 8/150... Loss: 1.2236354351043701\n",
            "Epoch: 9/150... Loss: 1.2116458415985107\n",
            "Epoch: 10/150... Loss: 1.2217319011688232\n",
            "Epoch: 11/150... Loss: 1.2309726476669312\n",
            "Epoch: 12/150... Loss: 1.2315723896026611\n",
            "Epoch: 13/150... Loss: 1.214572548866272\n",
            "Epoch: 14/150... Loss: 1.2257651090621948\n",
            "Epoch: 15/150... Loss: 1.2202945947647095\n",
            "Epoch: 16/150... Loss: 1.2133337259292603\n",
            "Epoch: 17/150... Loss: 1.2101211547851562\n",
            "Epoch: 18/150... Loss: 1.2197320461273193\n",
            "Epoch: 19/150... Loss: 1.206028938293457\n",
            "Epoch: 20/150... Loss: 1.2119054794311523\n",
            "Epoch: 21/150... Loss: 1.1954658031463623\n",
            "Epoch: 22/150... Loss: 1.1985055208206177\n",
            "Epoch: 23/150... Loss: 1.1853152513504028\n",
            "Epoch: 24/150... Loss: 1.2157864570617676\n",
            "Epoch: 25/150... Loss: 1.1914058923721313\n",
            "Epoch: 26/150... Loss: 1.1874003410339355\n",
            "Epoch: 27/150... Loss: 1.1900027990341187\n",
            "Epoch: 28/150... Loss: 1.162971019744873\n",
            "Epoch: 29/150... Loss: 1.1651438474655151\n",
            "Epoch: 30/150... Loss: 1.1942336559295654\n",
            "Epoch: 31/150... Loss: 1.164244532585144\n",
            "Epoch: 32/150... Loss: 1.1917473077774048\n",
            "Epoch: 33/150... Loss: 1.1864914894104004\n",
            "Epoch: 34/150... Loss: 1.171146273612976\n",
            "Epoch: 35/150... Loss: 1.160057783126831\n",
            "Epoch: 36/150... Loss: 1.2035101652145386\n",
            "Epoch: 37/150... Loss: 1.1931333541870117\n",
            "Epoch: 38/150... Loss: 1.1777386665344238\n",
            "Epoch: 39/150... Loss: 1.1846041679382324\n",
            "Epoch: 40/150... Loss: 1.1783026456832886\n",
            "Epoch: 41/150... Loss: 1.1719698905944824\n",
            "Epoch: 42/150... Loss: 1.1902886629104614\n",
            "Epoch: 43/150... Loss: 1.1651023626327515\n",
            "Epoch: 44/150... Loss: 1.1725279092788696\n",
            "Epoch: 45/150... Loss: 1.1863040924072266\n",
            "Epoch: 46/150... Loss: 1.1491303443908691\n",
            "Epoch: 47/150... Loss: 1.1573787927627563\n",
            "Epoch: 48/150... Loss: 1.2332476377487183\n",
            "Epoch: 49/150... Loss: 1.1931227445602417\n",
            "Epoch: 50/150... Loss: 1.2278130054473877\n",
            "Epoch: 51/150... Loss: 1.1900156736373901\n",
            "Epoch: 52/150... Loss: 1.1534138917922974\n",
            "Epoch: 53/150... Loss: 1.1532148122787476\n",
            "Epoch: 54/150... Loss: 1.1640138626098633\n",
            "Epoch: 55/150... Loss: 1.1850192546844482\n",
            "Epoch: 56/150... Loss: 1.1646069288253784\n",
            "Epoch: 57/150... Loss: 1.1628119945526123\n",
            "Epoch: 58/150... Loss: 1.166379690170288\n",
            "Epoch: 59/150... Loss: 1.1587671041488647\n",
            "Epoch: 60/150... Loss: 1.158011794090271\n",
            "Epoch: 61/150... Loss: 1.1520466804504395\n",
            "Epoch: 62/150... Loss: 1.1881885528564453\n",
            "Epoch: 63/150... Loss: 1.1865938901901245\n",
            "Epoch: 64/150... Loss: 1.167966365814209\n",
            "Epoch: 65/150... Loss: 1.159989833831787\n",
            "Epoch: 66/150... Loss: 1.1600687503814697\n",
            "Epoch: 67/150... Loss: 1.1725130081176758\n",
            "Epoch: 68/150... Loss: 1.167626976966858\n",
            "Epoch: 69/150... Loss: 1.1815379858016968\n",
            "Epoch: 70/150... Loss: 1.1551913022994995\n",
            "Epoch: 71/150... Loss: 1.189894437789917\n",
            "Epoch: 72/150... Loss: 1.1755130290985107\n",
            "Epoch: 73/150... Loss: 1.1637097597122192\n",
            "Epoch: 74/150... Loss: 1.1804490089416504\n",
            "Epoch: 75/150... Loss: 1.1782169342041016\n",
            "Epoch: 76/150... Loss: 1.1686773300170898\n",
            "Epoch: 77/150... Loss: 1.1560856103897095\n",
            "Epoch: 78/150... Loss: 1.157078504562378\n",
            "Epoch: 79/150... Loss: 1.1699795722961426\n",
            "Epoch: 80/150... Loss: 1.158177137374878\n",
            "Epoch: 81/150... Loss: 1.158574104309082\n",
            "Epoch: 82/150... Loss: 1.1421328783035278\n",
            "Epoch: 83/150... Loss: 1.198249340057373\n",
            "Epoch: 84/150... Loss: 1.1983189582824707\n",
            "Epoch: 85/150... Loss: 1.2198039293289185\n",
            "Epoch: 86/150... Loss: 1.2920624017715454\n",
            "Epoch: 87/150... Loss: 1.2514667510986328\n",
            "Epoch: 88/150... Loss: 1.2595932483673096\n",
            "Epoch: 89/150... Loss: 1.3202736377716064\n",
            "Epoch: 90/150... Loss: 1.3992462158203125\n",
            "Epoch: 91/150... Loss: 1.3215941190719604\n",
            "Epoch: 92/150... Loss: 1.3763782978057861\n",
            "Epoch: 93/150... Loss: 1.305902361869812\n",
            "Epoch: 94/150... Loss: 1.2902617454528809\n",
            "Epoch: 95/150... Loss: 1.3045252561569214\n",
            "Epoch: 96/150... Loss: 1.2892029285430908\n",
            "Epoch: 97/150... Loss: 1.257449746131897\n",
            "Epoch: 98/150... Loss: 1.3165186643600464\n",
            "Epoch: 99/150... Loss: 1.3677467107772827\n",
            "Epoch: 100/150... Loss: 1.3613996505737305\n",
            "Epoch: 101/150... Loss: 1.3765796422958374\n",
            "Epoch: 102/150... Loss: 2.914576530456543\n",
            "Epoch: 103/150... Loss: 2.8253533840179443\n",
            "Epoch: 104/150... Loss: 3.6345396041870117\n",
            "Epoch: 105/150... Loss: 3.9392950534820557\n",
            "Epoch: 106/150... Loss: 3.672788619995117\n",
            "Epoch: 107/150... Loss: 3.6831259727478027\n",
            "Epoch: 108/150... Loss: 3.5202951431274414\n",
            "Epoch: 109/150... Loss: 3.6141774654388428\n",
            "Epoch: 110/150... Loss: 3.656446933746338\n",
            "Epoch: 111/150... Loss: 3.5659596920013428\n",
            "Epoch: 112/150... Loss: 3.6366612911224365\n",
            "Epoch: 113/150... Loss: 3.7454092502593994\n",
            "Epoch: 114/150... Loss: 3.5187530517578125\n",
            "Epoch: 115/150... Loss: 3.667919874191284\n",
            "Epoch: 116/150... Loss: 3.5045723915100098\n",
            "Epoch: 117/150... Loss: 3.6639163494110107\n",
            "Epoch: 118/150... Loss: 3.5785815715789795\n",
            "Epoch: 119/150... Loss: 3.582879066467285\n",
            "Epoch: 120/150... Loss: 3.6151559352874756\n",
            "Epoch: 121/150... Loss: 3.621584892272949\n",
            "Epoch: 122/150... Loss: 3.6277353763580322\n",
            "Epoch: 123/150... Loss: 3.77266001701355\n",
            "Epoch: 124/150... Loss: 3.7199642658233643\n",
            "Epoch: 125/150... Loss: 3.521573543548584\n",
            "Epoch: 126/150... Loss: 3.49222469329834\n",
            "Epoch: 127/150... Loss: 3.8021469116210938\n",
            "Epoch: 128/150... Loss: 3.605086088180542\n",
            "Epoch: 129/150... Loss: 3.4960806369781494\n",
            "Epoch: 130/150... Loss: 3.609549045562744\n",
            "Epoch: 131/150... Loss: 3.5503952503204346\n",
            "Epoch: 132/150... Loss: 3.6275131702423096\n",
            "Epoch: 133/150... Loss: 3.5871591567993164\n",
            "Epoch: 134/150... Loss: 3.5912585258483887\n",
            "Epoch: 135/150... Loss: 3.6115097999572754\n",
            "Epoch: 136/150... Loss: 3.5435376167297363\n",
            "Epoch: 137/150... Loss: 3.5727996826171875\n",
            "Epoch: 138/150... Loss: 3.608158588409424\n",
            "Epoch: 139/150... Loss: 3.7315542697906494\n",
            "Epoch: 140/150... Loss: 3.541750192642212\n",
            "Epoch: 141/150... Loss: 3.5489537715911865\n",
            "Epoch: 142/150... Loss: 3.6278562545776367\n",
            "Epoch: 143/150... Loss: 3.758195400238037\n",
            "Epoch: 144/150... Loss: 3.5422050952911377\n",
            "Epoch: 145/150... Loss: 3.565847396850586\n",
            "Epoch: 146/150... Loss: 3.56758189201355\n",
            "Epoch: 147/150... Loss: 3.4875357151031494\n",
            "Epoch: 148/150... Loss: 3.630401372909546\n",
            "Epoch: 149/150... Loss: 3.6290557384490967\n",
            "Epoch: 150/150... Loss: 3.7306718826293945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar el modelo entrenado\n",
        "model_save_path = \"lstm_cuentos_infantiles_150epoch.pth\"\n",
        "torch.save(model.state_dict(), model_save_path)"
      ],
      "metadata": {
        "id": "ZdxvQRVbitAR"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prime: Texto inicial con el que comienza la generación de nuevo texto.\n",
        "# ---- Cambiar aqui para pruebas c: ----\n",
        "prime = \"Once upon a time\""
      ],
      "metadata": {
        "id": "KlBAGt9A-aK5"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar un cuento\n",
        "#generated_text = sample(model, 500)\n",
        "generated_text = sample3(model, 1000, device, prime=prime)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "_hcQa8kqWQg-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c921c741-c883-4bd2-baae-1a4a68a6cbf6"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time; be on s ann, and stanehy nute dar bigelen hatterga  the ho s   And the statu shall of sin sheds was; and woe  tirs, that son of the crest, and ware  the tail would dee s  humtinge to break it and ooas. the sun sprint of then wound awah, wound ensito aetse t trees toh hra  h toinnu toe he ha st ahee,h ar,t hean had ene, drthy seoueth en as ans to gaysno a so s wastede a gooe rris e da  sa todurtoe ste woo ss ant\n",
            "e   shhe  ouu at ohe a  he weatfn bacen beneernat. Thomethe ofe as ane drewe shou  tilkitt sea hee wine th breaphunente shh ahuthe  to min, as sakid he thouch. When he cise shel  wild by ant sharn.\n",
            " Thou are su daite wes so oonea tok to wilse teo dw fto ne site. \n",
            "T he had a goon heat oa. I sum the stead is no moute. If you done help hals, the midnans, a leet her on  tet \". de heer onst htha sene a sadisentont the ants  it a boom.  I am tiskfore a,  \"That arl we,\" said the comair of liferaieec. “Woros to sen,” he said. “He is thie, and the other and of ath. “Wote sur,” ans, “I w\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "a7rTa4rfVisW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gCkaTSaDzfbe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}